# Paper Note 

# Topic

## VQA

### VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts
### LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection
### Flamingo: a Visual Language Model for Few-Shot Learning
### Sparse and Continuous Attention Mechanisms - experiments on VQA with continuous attention
### ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES
### RUBi: Reducing Unimodal Biases in Visual Question Answering (to read)
### Compact Trilinear Interaction for Visual Question Answering (to read)
### MUREL: Multimodal Relational Reasoning for Visual Question Answering (to read)
### [Deep Modular Co-Attention Networks for Visual Question Answering](https://github.com/MILVLG/mcan-vqa) (MCAN) (cite, learned paper and code, to read experiment)
### [MUTAN: Multimodal Tucker Fusion for Visual Question Answering](https://github.com/Cadene/vqa.pytorch) (cite)
### [Hadamard Product for Low-rank Bilinear Pooling](https://github.com/jnhwkim/MulLowBiVQA) (MLB)
### [Bilinear Attention Networks](https://github.com/jnhwkim/ban-vqa) (BAN) (learned paper and code) (cite)
### [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](https://github.com/akirafukui/vqa-mcb) (MCB) (cite? learned paper)
### [BLOCK: Bilinear Superdiagonal Fusion for VQA and VRD](https://github.com/Cadene/block.bootstrap.pytorch/tree/master) (AAAI 2019) (cite, to read! code includes many other models)
### Invertible Question Answering Network (iQAN) 
### [Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering](https://github.com/yuzcccc/vqa-mfb) (MFB) (to read)
### [Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling for Visual Question Answering](https://github.com/yuzcccc/vqa-mfb) (MFH) (to read)
### [LEARNING TO COUNT OBJECTS IN NATURAL IMAGES FOR VISUAL QUESTION ANSWERING](https://github.com/Cyanogenoid/vqa-counting) (learned paper and code)
### [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](https://arxiv.org/pdf/1707.07998)
### [Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge](https://arxiv.org/pdf/1708.02711)
### [Learning Conditioned Graph Structures for Interpretable Visual Question Answering](https://arxiv.org/pdf/1806.07243)
### [Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings](https://arxiv.org/pdf/1804.11146)
### [Deep Multimodal Learning for Vision and Language Processing](http://remicadene.com/pdfs/thesis.pdf)
### [VQA Survery](https://github.com/BDBC-KG-NLP/QA-Survey-CN)


## Multimodal

### [Foundations & Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions](https://arxiv.org/pdf/2209.03430)
### [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/pdf/2102.03334)
### [LANGUAGE-DRIVEN SEMANTIC SEGMENTATION](https://arxiv.org/pdf/2201.03546)(Lseg)
### [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/pdf/2202.11094)(GroupViT)
### [OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION](https://arxiv.org/pdf/2104.13921)(ViLD)
### [Grounded Language-Image Pre-training](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf)(GLIP)
### [CLIPasso: Semantically-Aware Object Sketching](https://arxiv.org/pdf/2202.05822)
### [CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval](https://arxiv.org/pdf/2104.08860)
### [ActionCLIP: A New Paradigm for Video Action Recognition](https://arxiv.org/pdf/2109.08472)
### [How Much Can CLIP Benefit Vision-and-Language Tasks?](https://arxiv.org/pdf/2107.06383)
### [PointCLIP: Point Cloud Understanding by CLIP](https://arxiv.org/pdf/2112.02413)
### [Can Language Understand Depth?](https://arxiv.org/pdf/2207.01077)
### [GAIA: A Fine-grained Multimedia Knowledge Extraction System](https://aclanthology.org/2020.acl-demos.11.pdf)
### [TEXTEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction]
### [SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark](https://arxiv.org/pdf/2402.05138)

### To read
### [TRINS: Towards Multimodal Language Models that Can Read](https://arxiv.org/pdf/2406.06730)

## Dataset

### [Visual Genome Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://arxiv.org/pdf/1602.07332)
### [MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos](https://arxiv.org/pdf/2306.04216)

## Video

### dataset: UCF101, K400
### [Learning Spatiotemporal Features with 3D Convolutional Networks](https://arxiv.org/pdf/1412.0767) (没有使用光流，使用3D网络。基于VGG)
### [Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset](https://arxiv.org/pdf/1705.07750) （扩充ImageNet上面的参数到3D）
### [Non-local Neural Networks](https://arxiv.org/pdf/1711.07971) (加入Attention Block(QKV))
### [A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://arxiv.org/pdf/1711.11248) (R(2+1)D, 把3D网络拆封成空间2D，时间1D)
### [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982) (两个分支：一个帧率高消耗小，一个帧率低消耗高)
### [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/pdf/2102.05095) (Timesformer: vision transformer 类似到 video transformer, 各种做自注意力方法)
### [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125) (DALL.E2)

## 3D

### [NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video](https://zju3dv.github.io/neuralrecon/)

### To read
### [MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval](https://arxiv.org/pdf/2301.07868)

## Fairness

### [AI FAIRNESS 360: AN EXTENSIBLE TOOLKIT FOR DETECTING, UNDERSTANDING, AND MITIGATING UNWANTED ALGORITHMIC BIAS](https://arxiv.org/pdf/1810.01943)(AIF 360)
### [Discover and Mitigate Multiple Biased Subgroups in Image Classifiers](https://arxiv.org/pdf/2403.12777)

### To read
### [FairCLIP: Harnessing Fairness in Vision-Language Learning](https://arxiv.org/pdf/2403.19949)
### [FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication](https://arxiv.org/pdf/2404.16123)
### [Classes Are Not Equal: An Empirical Study on Image Recognition Fairness](https://arxiv.org/pdf/2402.18133)

## Information Retrieval

### [A Neural Corpus Indexer for Document Retrieval](https://arxiv.org/pdf/2206.02743)

## AI + Tool

### [ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph](https://arxiv.org/pdf/2403.00839)
### [ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models]

# Person

## VT Lifu Huang
### [MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=76IEGtYAAAAJ&sortby=pubdate&citation_for_view=76IEGtYAAAAJ:fQNAKQ3IYiAC) (comment: a new database)
### [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning]
