# vqa

## paper note

### VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts
### LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection
### Flamingo: a Visual Language Model for Few-Shot Learning
### Sparse and Continuous Attention Mechanisms - experiments on VQA with continuous attention
### ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES
### RUBi: Reducing Unimodal Biases in Visual Question Answering (to read)
### Compact Trilinear Interaction for Visual Question Answering (to read)
### MUREL: Multimodal Relational Reasoning for Visual Question Answering (to read)
### Deep Modular Co-Attention Networks for Visual Question Answering (to read experiment)
### [MUTAN: Multimodal Tucker Fusion for Visual Question Answering] (https://github.com/Cadene/vqa.pytorch)
### Bilinear Attention Networks
### Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding
### BLOCK: Bilinear Superdiagonal Fusion for VQA and VRD (to read!)
### Invertible Question Answering Network (iQAN) 

