# paper note 

## VQA

### VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts
### LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection
### Flamingo: a Visual Language Model for Few-Shot Learning
### Sparse and Continuous Attention Mechanisms - experiments on VQA with continuous attention
### ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES
### RUBi: Reducing Unimodal Biases in Visual Question Answering (to read)
### Compact Trilinear Interaction for Visual Question Answering (to read)
### MUREL: Multimodal Relational Reasoning for Visual Question Answering (to read)
### [Deep Modular Co-Attention Networks for Visual Question Answering](https://github.com/MILVLG/mcan-vqa) (MCAN) (learned paper and code, to read experiment)
### [MUTAN: Multimodal Tucker Fusion for Visual Question Answering](https://github.com/Cadene/vqa.pytorch)
### [Hadamard Product for Low-rank Bilinear Pooling](https://github.com/jnhwkim/MulLowBiVQA) (MLB)
### [Bilinear Attention Networks](https://github.com/jnhwkim/ban-vqa) (BAN) (learned paper and code)
### Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding (MCB) (learned paper)
### [BLOCK: Bilinear Superdiagonal Fusion for VQA and VRD](https://github.com/Cadene/block.bootstrap.pytorch/tree/master) (AAAI 2019) (to read! code includes many other models)
### Invertible Question Answering Network (iQAN) 
### [Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering](https://github.com/yuzcccc/vqa-mfb) (MFB) (to read)
### [Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling for Visual Question Answering](https://github.com/yuzcccc/vqa-mfb) (MFH) (to read)
### [LEARNING TO COUNT OBJECTS IN NATURAL IMAGES FOR VISUAL QUESTION ANSWERING](https://github.com/Cyanogenoid/vqa-counting) (learned paper and code)


## Multimodal

### [Foundations & Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions](https://arxiv.org/pdf/2209.03430)

## VT Lifu Huang
### [MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=76IEGtYAAAAJ&sortby=pubdate&citation_for_view=76IEGtYAAAAJ:fQNAKQ3IYiAC) (comment: a new database)
