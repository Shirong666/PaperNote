# vqa

## paper note

### VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts
### LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection
### Flamingo: a Visual Language Model for Few-Shot Learning
### Sparse and Continuous Attention Mechanisms - experiments on VQA with continuous attention
### ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES
### RUBi: Reducing Unimodal Biases in Visual Question Answering (to read)
### Compact Trilinear Interaction for Visual Question Answering (to read)
### MUREL: Multimodal Relational Reasoning for Visual Question Answering (to read)
### [Deep Modular Co-Attention Networks for Visual Question Answering](https://github.com/MILVLG/mcan-vqa) (MCAN) (learned paper and code, to read experiment)
### [MUTAN: Multimodal Tucker Fusion for Visual Question Answering](https://github.com/Cadene/vqa.pytorch)
### [Hadamard Product for Low-rank Bilinear Pooling](https://github.com/jnhwkim/MulLowBiVQA) (MLB)
### [Bilinear Attention Networks](https://github.com/jnhwkim/ban-vqa) (BAN) (learned paper and code)
### Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding (MCB) (learned paper)
### BLOCK: Bilinear Superdiagonal Fusion for VQA and VRD (to read!)
### Invertible Question Answering Network (iQAN) 
### [Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering](https://github.com/yuzcccc/vqa-mfb) (MFB) (to read)
### [Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling for Visual Question Answering](https://github.com/yuzcccc/vqa-mfb) (MFH) (to read)
### [LEARNING TO COUNT OBJECTS IN NATURAL IMAGES FOR VISUAL QUESTION ANSWERING](https://github.com/Cyanogenoid/vqa-counting) (learned paper and code)

